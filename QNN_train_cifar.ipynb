{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "from torch import cat, no_grad, manual_seed\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from skimage.color import rgb2gray \n",
    "\n",
    "from QuICT_ml.ansatz_library import QNNLayer\n",
    "from QuICT_ml.utils.encoding import *\n",
    "from QuICT_ml.utils.ml_utils import *\n",
    "from QuICT_ml.model.QNN import QuantumNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "manual_seed(42)\n",
    "EPOCH = 50       # 训练总轮数\n",
    "BATCH_SIZE = 64 # 一次迭代使用的样本数\n",
    "LR = 0.001      # 梯度下降的学习率\n",
    "SEED = 42       # 随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training examples:  2048\n",
      "Testing examples:  1024\n"
     ]
    }
   ],
   "source": [
    "# 加载CIFAR-10数据集\n",
    "X_train = datasets.CIFAR10(root=\"./data/\", train=True, download=True)\n",
    "X_test = datasets.CIFAR10(root=\"./data/\", train=False, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "n_samples = 1024  # 我们只关注前1024个样本\n",
    "\n",
    "# 将3通道RGB图像转换为单通道灰度图像\n",
    "def convert_to_grayscale(data):\n",
    "    grayscale_data = np.array([rgb2gray(img) for img in data])  # 转换为灰度图像\n",
    "    grayscale_data = grayscale_data[:, :, :, np.newaxis]  # 增加一个维度，形状从 (N, 32, 32) 变为 (N, 32, 32, 1)\n",
    "    return grayscale_data\n",
    "\n",
    "# 创建一个索引列表，包含所有类别（0-9）的样本\n",
    "idx = []\n",
    "for label in range(2):  # 遍历前两个类别（0和1）\n",
    "    label_idx = torch.where(torch.tensor(X_train.targets) == label)[0][:n_samples]  # 获取当前类别的样本索引\n",
    "    idx.append(label_idx)\n",
    "\n",
    "# 将所有类别的索引合并为一个数组\n",
    "idx = torch.cat(idx)\n",
    "\n",
    "# 根据索引过滤数据\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = [X_train.targets[i] for i in idx]  # 注意：targets 是列表，不是张量\n",
    "\n",
    "# 将训练数据转换为灰度图像\n",
    "train_X = convert_to_grayscale(X_train.data)\n",
    "train_Y = X_train.targets\n",
    "\n",
    "n_samples = 512\n",
    "idx = []\n",
    "for label in range(2):  # 遍历前两个类别（0和1）\n",
    "    label_idx = torch.where(torch.tensor(X_test.targets) == label)[0][:n_samples]  # 获取当前类别的样本索引\n",
    "    idx.append(label_idx)\n",
    "\n",
    "# 将所有类别的索引合并为一个数组\n",
    "idx = torch.cat(idx)\n",
    "\n",
    "# 根据索引过滤数据\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = [X_test.targets[i] for i in idx]  # 注意：targets 是列表，不是张量\n",
    "\n",
    "# 将测试数据转换为灰度图像\n",
    "test_X = convert_to_grayscale(X_test.data)\n",
    "test_Y = X_test.targets\n",
    "\n",
    "print(\"Training examples: \", len(train_Y))\n",
    "print(\"Testing examples: \", len(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized training data shape:  torch.Size([2048, 4, 4])\n",
      "Resized testing data shape:  torch.Size([1024, 4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ly\\.conda\\envs\\Qenv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 定义下采样函数\n",
    "def downscale(X, resize):\n",
    "    # 将 NumPy 数组转换为 PyTorch 张量\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.from_numpy(X).float()  # 转换为浮点型张量\n",
    "    # 调整维度顺序：从 (N, H, W, C) 到 (N, C, H, W)\n",
    "    if X.ndim == 4:  # 如果是批量数据\n",
    "        X = X.permute(0, 3, 1, 2)  # 将通道维度移到第2维\n",
    "    elif X.ndim == 3:  # 如果是单张图像\n",
    "        X = X.permute(2, 0, 1)  # 将通道维度移到第1维\n",
    "    # 定义 Resize 变换\n",
    "    transform = transforms.Resize(size=resize)\n",
    "    # 对每张图像进行下采样\n",
    "    X = transform(X) / 255.0  # 下采样并归一化\n",
    "    X = torch.squeeze(X, dim=1)  # 删除通道维度\n",
    "    return X\n",
    "\n",
    "# 示例数据\n",
    "# train_X 和 test_X 是 NumPy 数组，形状为 (N, 32, 32, 1)\n",
    "resized_train_X = downscale(train_X, (4, 4))\n",
    "resized_test_X = downscale(test_X, (4, 4))\n",
    "\n",
    "print(\"Resized training data shape: \", resized_train_X.shape)\n",
    "print(\"Resized testing data shape: \", resized_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining training examples:  2048\n",
      "Remaining testing examples:  1024\n"
     ]
    }
   ],
   "source": [
    "def remove_conflict(X, Y, resize):\n",
    "    x_dict = collections.defaultdict(set)\n",
    "    for x, y in zip(X, Y):\n",
    "        x_dict[tuple(x.numpy().flatten())].add(y)\n",
    "    X_rmcon = []\n",
    "    Y_rmcon = []\n",
    "    for x in x_dict.keys():\n",
    "        if len(x_dict[x]) == 1:\n",
    "            X_rmcon.append(np.array(x).reshape(resize))\n",
    "            Y_rmcon.append(list(x_dict[x])[0])\n",
    "    X = torch.from_numpy(np.array(X_rmcon))\n",
    "    Y = torch.from_numpy(np.array(Y_rmcon))\n",
    "    return X, Y\n",
    "\n",
    "nocon_train_X, nocon_train_Y = remove_conflict(resized_train_X, train_Y, (4, 4))\n",
    "nocon_test_X, nocon_test_Y = remove_conflict(resized_test_X, test_Y, (4, 4))\n",
    "print(\"Remaining training examples: \", len(nocon_train_Y))\n",
    "print(\"Remaining testing examples: \", len(nocon_test_Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_img(X, threshold):\n",
    "    X = X > threshold\n",
    "    X = X.type(torch.int)\n",
    "    return X\n",
    "\n",
    "threshold = 0.5\n",
    "bin_train_X = binary_img(nocon_train_X, threshold)\n",
    "bin_test_X = binary_img(nocon_test_X, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "train_X = bin_train_X.to(device)\n",
    "train_Y = nocon_train_Y.to(device)\n",
    "test_X = bin_test_X.to(device)\n",
    "test_Y = nocon_test_Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qubit_encoding(X, device):\n",
    "    new_X = []\n",
    "    n_qubits = X[0].shape[0] * X[0].shape[1]\n",
    "    qe = Qubit(n_qubits, device)\n",
    "    for x in X:\n",
    "        qe.encoding(x)\n",
    "        new_X.append(qe.ansatz)\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz_train_X = qubit_encoding(train_X, device)\n",
    "ansatz_test_X = qubit_encoding(test_X, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 848.056x645 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pqc = QNNLayer(list(range(4)), 4, device=device)\n",
    "params = nn.Parameter(torch.rand(1, 4, device=device), requires_grad=True)\n",
    "model_circuit = pqc.circuit_layer([\"XX\"], params)\n",
    "model_circuit.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qubits = list(range(16))\n",
    "readout_qubit = 16\n",
    "pqc = QNNLayer(data_qubits, readout_qubit, device=device)\n",
    "layers = [\"XX\", \"ZZ\"]\n",
    "params = nn.Parameter(torch.rand(2, 16, device=device), requires_grad=True)\n",
    "model_ansatz = pqc(layers, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.TensorDataset(train_X, train_Y)\n",
    "test_dataset = data.TensorDataset(test_X, test_Y)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = QuantumNet(16, layers, encoding=\"qubit\", device=device)\n",
    "optim = torch.optim.Adam([dict(params=net.parameters(), lr=LR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    y_true = 2 * y_true.type(torch.float32) - 1.0\n",
    "    y_pred = 2 * y_pred - 1.0\n",
    "    loss = torch.clamp(1 - y_pred * y_true, min=0.0)\n",
    "    correct = torch.where(y_true * y_pred > 0)[0].shape[0]\n",
    "    return torch.mean(loss), correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 32/32 [13:04<00:00, 24.52s/it, accuracy=0.531, it=31, loss=0.982]\n",
      "Validating epoch 1: 100%|██████████| 16/16 [06:27<00:00, 24.23s/it, accuracy=0.484, it=15, loss=1.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average Loss: 1.0, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 32/32 [12:49<00:00, 24.05s/it, accuracy=0.453, it=31, loss=1.028]\n",
      "Training epoch 3: 100%|██████████| 32/32 [12:46<00:00, 23.96s/it, accuracy=0.562, it=31, loss=0.964]\n",
      "Training epoch 4: 100%|██████████| 32/32 [12:48<00:00, 24.03s/it, accuracy=0.547, it=31, loss=0.974]\n",
      "Training epoch 5: 100%|██████████| 32/32 [12:48<00:00, 24.03s/it, accuracy=0.516, it=31, loss=0.991]\n",
      "Training epoch 6: 100%|██████████| 32/32 [12:48<00:00, 24.03s/it, accuracy=0.516, it=31, loss=0.990]\n",
      "Training epoch 7: 100%|██████████| 32/32 [12:48<00:00, 24.03s/it, accuracy=0.625, it=31, loss=0.923]\n",
      "Training epoch 8: 100%|██████████| 32/32 [12:48<00:00, 24.02s/it, accuracy=0.656, it=31, loss=0.909]\n",
      "Training epoch 9: 100%|██████████| 32/32 [12:48<00:00, 24.01s/it, accuracy=0.531, it=31, loss=0.982]\n",
      "Training epoch 10: 100%|██████████| 32/32 [12:46<00:00, 23.95s/it, accuracy=0.500, it=31, loss=1.000]\n",
      "Training epoch 11: 100%|██████████| 32/32 [12:48<00:00, 24.01s/it, accuracy=0.484, it=31, loss=1.010]\n",
      "Validating epoch 11: 100%|██████████| 16/16 [06:26<00:00, 24.17s/it, accuracy=0.406, it=15, loss=1.060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average Loss: 1.0, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 12: 100%|██████████| 32/32 [12:51<00:00, 24.12s/it, accuracy=0.547, it=31, loss=0.971]\n",
      "Training epoch 13: 100%|██████████| 32/32 [12:49<00:00, 24.06s/it, accuracy=0.516, it=31, loss=0.990]\n",
      "Training epoch 14: 100%|██████████| 32/32 [12:51<00:00, 24.12s/it, accuracy=0.453, it=31, loss=1.029]\n",
      "Training epoch 15: 100%|██████████| 32/32 [12:51<00:00, 24.09s/it, accuracy=0.516, it=31, loss=0.990]\n",
      "Training epoch 16: 100%|██████████| 32/32 [12:51<00:00, 24.11s/it, accuracy=0.547, it=31, loss=0.971]\n",
      "Training epoch 17: 100%|██████████| 32/32 [12:51<00:00, 24.11s/it, accuracy=0.500, it=31, loss=1.000]\n",
      "Training epoch 18: 100%|██████████| 32/32 [12:54<00:00, 24.20s/it, accuracy=0.484, it=31, loss=1.009]\n",
      "Training epoch 19: 100%|██████████| 32/32 [12:50<00:00, 24.08s/it, accuracy=0.406, it=31, loss=1.062]\n",
      "Training epoch 20: 100%|██████████| 32/32 [12:49<00:00, 24.04s/it, accuracy=0.406, it=31, loss=1.061]\n",
      "Training epoch 21: 100%|██████████| 32/32 [12:49<00:00, 24.05s/it, accuracy=0.562, it=31, loss=0.964]\n",
      "Validating epoch 21: 100%|██████████| 16/16 [06:23<00:00, 23.96s/it, accuracy=0.500, it=15, loss=1.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average Loss: 1.0, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 22: 100%|██████████| 32/32 [12:50<00:00, 24.07s/it, accuracy=0.516, it=31, loss=0.991]\n",
      "Training epoch 23: 100%|██████████| 32/32 [12:51<00:00, 24.11s/it, accuracy=0.516, it=31, loss=0.990]\n",
      "Training epoch 24: 100%|██████████| 32/32 [12:50<00:00, 24.08s/it, accuracy=0.500, it=31, loss=1.000]\n",
      "Training epoch 25: 100%|██████████| 32/32 [12:50<00:00, 24.08s/it, accuracy=0.547, it=31, loss=0.973]\n",
      "Training epoch 26: 100%|██████████| 32/32 [12:49<00:00, 24.06s/it, accuracy=0.422, it=31, loss=1.051]\n",
      "Training epoch 27: 100%|██████████| 32/32 [12:52<00:00, 24.13s/it, accuracy=0.438, it=31, loss=1.040]\n",
      "Training epoch 28: 100%|██████████| 32/32 [12:50<00:00, 24.09s/it, accuracy=0.500, it=31, loss=1.000]\n",
      "Training epoch 29: 100%|██████████| 32/32 [12:50<00:00, 24.09s/it, accuracy=0.484, it=31, loss=1.010]\n",
      "Training epoch 30: 100%|██████████| 32/32 [12:50<00:00, 24.07s/it, accuracy=0.531, it=31, loss=0.980]\n",
      "Training epoch 31: 100%|██████████| 32/32 [12:51<00:00, 24.11s/it, accuracy=0.422, it=31, loss=1.051]\n",
      "Validating epoch 31: 100%|██████████| 16/16 [06:23<00:00, 23.97s/it, accuracy=0.531, it=15, loss=0.980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average Loss: 1.0, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 32: 100%|██████████| 32/32 [12:58<00:00, 24.34s/it, accuracy=0.562, it=31, loss=0.961]\n",
      "Training epoch 33: 100%|██████████| 32/32 [12:52<00:00, 24.13s/it, accuracy=0.484, it=31, loss=1.010]\n",
      "Training epoch 34: 100%|██████████| 32/32 [12:50<00:00, 24.08s/it, accuracy=0.516, it=31, loss=0.989]\n",
      "Training epoch 35: 100%|██████████| 32/32 [12:51<00:00, 24.11s/it, accuracy=0.469, it=31, loss=1.019]\n",
      "Training epoch 36: 100%|██████████| 32/32 [12:50<00:00, 24.07s/it, accuracy=0.438, it=31, loss=1.043]\n",
      "Training epoch 37: 100%|██████████| 32/32 [12:50<00:00, 24.09s/it, accuracy=0.484, it=31, loss=1.010]\n",
      "Training epoch 38: 100%|██████████| 32/32 [12:50<00:00, 24.09s/it, accuracy=0.500, it=31, loss=1.000]\n",
      "Training epoch 39: 100%|██████████| 32/32 [12:50<00:00, 24.06s/it, accuracy=0.484, it=31, loss=1.011]\n",
      "Training epoch 40: 100%|██████████| 32/32 [12:50<00:00, 24.07s/it, accuracy=0.484, it=31, loss=1.010]\n",
      "Training epoch 41: 100%|██████████| 32/32 [12:50<00:00, 24.09s/it, accuracy=0.469, it=31, loss=1.021]\n",
      "Validating epoch 41: 100%|██████████| 16/16 [06:23<00:00, 23.96s/it, accuracy=0.516, it=15, loss=0.990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average Loss: 1.0, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 42: 100%|██████████| 32/32 [12:49<00:00, 24.04s/it, accuracy=0.609, it=31, loss=0.929]\n",
      "Training epoch 43: 100%|██████████| 32/32 [12:49<00:00, 24.03s/it, accuracy=0.469, it=31, loss=1.021]\n",
      "Training epoch 44: 100%|██████████| 32/32 [12:50<00:00, 24.09s/it, accuracy=0.422, it=31, loss=1.054]\n",
      "Training epoch 45: 100%|██████████| 32/32 [12:49<00:00, 24.04s/it, accuracy=0.406, it=31, loss=1.066]\n",
      "Training epoch 46: 100%|██████████| 32/32 [12:49<00:00, 24.05s/it, accuracy=0.500, it=31, loss=1.000]\n",
      "Training epoch 47: 100%|██████████| 32/32 [12:50<00:00, 24.08s/it, accuracy=0.453, it=31, loss=1.032]\n",
      "Training epoch 48: 100%|██████████| 32/32 [12:53<00:00, 24.18s/it, accuracy=0.531, it=31, loss=0.980]\n",
      "Training epoch 49: 100%|██████████| 32/32 [09:00<00:00, 16.90s/it, accuracy=0.469, it=31, loss=1.022]\n",
      "Training epoch 50: 100%|██████████| 32/32 [08:05<00:00, 15.16s/it, accuracy=0.547, it=31, loss=0.967]\n"
     ]
    }
   ],
   "source": [
    "# train epoch\n",
    "for ep in range(EPOCH):\n",
    "    net.train()\n",
    "    loader = tqdm.tqdm(\n",
    "        train_loader, desc=\"Training epoch {}\".format(ep + 1), leave=True\n",
    "    )\n",
    "    # train iteration\n",
    "    for it, (x_train, y_train) in enumerate(loader):\n",
    "        optim.zero_grad()\n",
    "        y_pred = net(x_train)\n",
    "\n",
    "        loss, correct = loss_func(y_train, y_pred)\n",
    "        accuracy = correct / len(y_train)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loader.set_postfix(\n",
    "            it=it,\n",
    "            loss=\"{:.3f}\".format(loss),\n",
    "            accuracy=\"{:.3f}\".format(accuracy),\n",
    "        )\n",
    "    if ep % 10 == 0:\n",
    "        # Validation\n",
    "        net.eval()\n",
    "        loader_val = tqdm.tqdm(\n",
    "            test_loader, desc=\"Validating epoch {}\".format(ep + 1), leave=True\n",
    "        )\n",
    "        loss_val_list = []\n",
    "        total_correct = 0\n",
    "        for it, (x_test, y_test) in enumerate(loader_val):\n",
    "            y_pred = net(x_test)\n",
    "            loss_val, correct = loss_func(y_test, y_pred)\n",
    "            loss_val_list.append(loss_val.cpu().detach().numpy())\n",
    "            total_correct += correct\n",
    "            accuracy_val = correct / len(y_test)\n",
    "            loader_val.set_postfix(\n",
    "                it=it,\n",
    "                loss=\"{:.3f}\".format(loss_val),\n",
    "                accuracy=\"{:.3f}\".format(accuracy_val),\n",
    "            )\n",
    "        avg_loss = np.mean(loss_val_list)\n",
    "        avg_acc = total_correct / (len(loader_val) * BATCH_SIZE)\n",
    "        print(\"Validation Average Loss: {}, Accuracy: {}\".format(avg_loss, avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
